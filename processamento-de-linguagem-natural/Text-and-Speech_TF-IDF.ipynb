{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Term Frequency (TF)\n",
    "Relative frequency of a term in a document\n",
    "    = term instances / total terms\n",
    "    \n",
    "## Inverse Document Frequency (IDF)\n",
    "Relative count of documents\n",
    "    = log(docs/docs with term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/mrcs/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100   630  100   630    0     0    542      0  0:00:01  0:00:01 --:--:--   543\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import nltk \n",
    "nltk.download(\"stopwords\")\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Pegando documento do Github e abrindo\n",
    "# Doc1\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Moon.txt -o Moon.txt\n",
    "    \n",
    "# Abrindo documento em modo de leitura e imprimindo seu conteudo\n",
    "doc1 = open(\"Moon.txt\", \"r\")\n",
    "doc1Txt = doc1.read()\n",
    "print(doc1Txt)\n",
    "# removendo numeros\n",
    "txt = ''.join(n for n in doc1Txt if not n.isdigit())\n",
    "\n",
    "# removendo pontuacao e convertendo todo o texto para minusculo\n",
    "txt = ''.join(n for n in txt if n not in punctuation).lower()\n",
    "txt = ' '.join([word for word in txt.split() if word not in (stopwords.words('english'))])\n",
    "\n",
    "# imprimindo o texto normalizado\n",
    "print(txt)\n",
    "print(\"--------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "# Doc2\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Gettysburg.txt -o Gettysburg.txt\n",
    "doc2 = open(\"Gettysburg.txt\", \"r\")\n",
    "doc2Txt = doc2.read()\n",
    "print(doc2Txt)\n",
    "txt2 = ''.join(n for n in doc2Txt if not n.isdigit())\n",
    "txt2 = ''.join(n for n in txt2 if n not in punctuation).lower()\n",
    "txt2 = ' '.join([word for word in txt2.split() if word not in (stopwords.words('english'))])\n",
    "print(txt2)\n",
    "print(\"--------------------------------------------------------------------------------------------------------\")\n",
    "\n",
    "# Doc3\n",
    "!curl https://raw.githubusercontent.com/MicrosoftLearning/AI-Introduction/master/files/Cognitive.txt -o Cognitive.txt\n",
    "doc3 = open(\"Cognitive.txt\", \"r\")\n",
    "doc3Txt = doc3.read()\n",
    "print(doc3Txt)\n",
    "txt3 = ''.join(n for n in doc3Txt if not n.isdigit())\n",
    "txt3 = ''.join(n for n in txt3 if n not in punctuation).lower()\n",
    "txt3 = ' '.join([word for word in txt3.split() if word not in (stopwords.words('english'))])\n",
    "print(txt3)\n",
    "print(\"---------------------------------------------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obter valores de TF-IDF para as três principais palavras em cada documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalar a biblioteza textblob e definir as funções TF-IDF\n",
    "\n",
    "#!pip install -U textblob\n",
    "import math\n",
    "from textblob import TextBlob as tb\n",
    "\n",
    "def tf(word, doc):\n",
    "    return doc.words.count(word) / len(doc.words)\n",
    "\n",
    "def contains(word, docs):\n",
    "    return sum(1 for doc in docs if word in doc.words)\n",
    "\n",
    "def idf(word, docs):\n",
    "    return math.log(len(docs) / (1 + contains(word, docs)))\n",
    "\n",
    "def tfidf(word, doc, docs):\n",
    "    return tf(word, doc) * idf(word, docs)\n",
    "\n",
    "# Cria a coleção de documentos com o textblob\n",
    "doc1 = tb(txt)\n",
    "doc2 = tb(txt2)\n",
    "doc3 = tb(txt3)\n",
    "docs = [doc1,doc2,doc3]\n",
    "\n",
    "# Use TF-IDF para obter as três mais importantes palavras de cada documento\n",
    "print(\"---------------------------------------------------------------------------------------------------------\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(\"Top palavras no documento: {}\".format(i + 1))\n",
    "    scores = {word: tfidf(word, doc, docs) for word in doc.words}\n",
    "    sorted_words = sorted(scores.items(), key=lambda x: x[1], reverse = True)\n",
    "    for word, score in sorted_words[:3]:\n",
    "        print(\"\\tPalavra: {}, TF-IDF: {}\".format(word, round(score,5)))\n",
    "    print(\"---------------------------------------------------------------------------------------------------------\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
